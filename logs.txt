
==> Audit <==
|-----------|-------------------|----------|--------------------|---------|----------------------|----------------------|
|  Command  |       Args        | Profile  |        User        | Version |      Start Time      |       End Time       |
|-----------|-------------------|----------|--------------------|---------|----------------------|----------------------|
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 09 May 24 15:57 CEST |                      |
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 09 May 24 16:01 CEST | 09 May 24 17:50 CEST |
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 09 May 24 17:58 CEST | 09 May 24 17:59 CEST |
| kubectl   | -- get po -A      | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 09 May 24 18:00 CEST |                      |
| dashboard |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 09 May 24 18:01 CEST |                      |
| dashboard |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 10:09 CEST |                      |
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 10:09 CEST |                      |
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 10:09 CEST | 10 May 24 10:11 CEST |
| dashboard |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 10:12 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 12:39 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 13:16 CEST |                      |
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 13:17 CEST | 10 May 24 13:18 CEST |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 13:18 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 13:22 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 13:38 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:03 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:22 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:23 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:33 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:34 CEST |                      |
| service   | php-service --url | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:38 CEST |                      |
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:45 CEST |                      |
| start     |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:45 CEST | 10 May 24 14:47 CEST |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:46 CEST |                      |
| dashboard |                   | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:48 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:48 CEST |                      |
| service   | php-service       | minikube | DESKTOP-MC6L7RF\HP | v1.33.0 | 10 May 24 14:57 CEST |                      |
|-----------|-------------------|----------|--------------------|---------|----------------------|----------------------|


==> Dernier démarrage <==
Log file created at: 2024/05/10 14:45:50
Running on machine: DESKTOP-MC6L7RF
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0510 14:45:50.121384   18892 out.go:291] Setting OutFile to fd 92 ...
I0510 14:45:50.121468   18892 out.go:338] TERM=,COLORTERM=, which probably does not support color
I0510 14:45:50.121468   18892 out.go:304] Setting ErrFile to fd 96...
I0510 14:45:50.121468   18892 out.go:338] TERM=,COLORTERM=, which probably does not support color
W0510 14:45:50.136644   18892 root.go:314] Error reading config file at C:\Users\HP\.minikube\config\config.json: open C:\Users\HP\.minikube\config\config.json: The system cannot find the file specified.
I0510 14:45:50.143461   18892 out.go:298] Setting JSON to false
I0510 14:45:50.148315   18892 start.go:129] hostinfo: {"hostname":"DESKTOP-MC6L7RF","uptime":609574,"bootTime":1714735575,"procs":292,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.4291 Build 19045.4291","kernelVersion":"10.0.19045.4291 Build 19045.4291","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"2c664777-4ed9-46a4-8757-1fc130e35b15"}
W0510 14:45:50.148315   18892 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0510 14:45:50.151314   18892 out.go:177] * minikube v1.33.0 sur Microsoft Windows 10 Pro 10.0.19045.4291 Build 19045.4291
I0510 14:45:50.154802   18892 notify.go:220] Checking for updates...
I0510 14:45:50.155797   18892 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0510 14:45:50.156349   18892 driver.go:392] Setting default libvirt URI to qemu:///system
I0510 14:45:52.411693   18892 out.go:177] * Utilisation du pilote hyperv basé sur le profil existant
I0510 14:45:52.413888   18892 start.go:297] selected driver: hyperv
I0510 14:45:52.413888   18892 start.go:901] validating driver "hyperv" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.21.205.249 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HP:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0510 14:45:52.413888   18892 start.go:912] status for hyperv: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0510 14:45:52.454063   18892 cni.go:84] Creating CNI manager for ""
I0510 14:45:52.454063   18892 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0510 14:45:52.454063   18892 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.21.205.249 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HP:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0510 14:45:52.454626   18892 iso.go:125] acquiring lock: {Name:mk081860909967f6fc5cdb6da2eb3d46f83b8876 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0510 14:45:52.459043   18892 out.go:177] * Démarrage du nœud "minikube" primary control-plane dans le cluster "minikube"
I0510 14:45:52.461237   18892 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0510 14:45:52.461237   18892 preload.go:147] Found local preload: C:\Users\HP\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0510 14:45:52.461237   18892 cache.go:56] Caching tarball of preloaded images
I0510 14:45:52.461776   18892 preload.go:173] Found C:\Users\HP\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0510 14:45:52.461776   18892 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0510 14:45:52.462336   18892 profile.go:143] Saving config to C:\Users\HP\.minikube\profiles\minikube\config.json ...
I0510 14:45:52.464888   18892 start.go:360] acquireMachinesLock for minikube: {Name:mk1c05ab924c8deb3db1ea833629a4c6f8f014f1 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0510 14:45:52.464888   18892 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0510 14:45:52.464888   18892 start.go:96] Skipping create...Using existing machine configuration
I0510 14:45:52.465395   18892 fix.go:54] fixHost starting: 
I0510 14:45:52.465442   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:45:53.387193   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:45:53.387193   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:45:53.387193   18892 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0510 14:45:53.387193   18892 fix.go:138] unexpected machine state, will restart: <nil>
I0510 14:45:53.389848   18892 out.go:177] * Mise à jour du VM hyperv en marche "minikube" ...
I0510 14:45:53.392211   18892 machine.go:94] provisionDockerMachine start ...
I0510 14:45:53.392211   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:45:54.368759   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:45:54.368759   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:45:54.368759   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:45:55.756129   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:45:55.756129   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:45:55.761988   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:45:55.762543   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:45:55.762543   18892 main.go:141] libmachine: About to run SSH command:
hostname
I0510 14:45:55.932955   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0510 14:45:55.932955   18892 buildroot.go:166] provisioning hostname "minikube"
I0510 14:45:55.932955   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:45:56.943143   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:45:56.943143   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:45:56.943143   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:45:58.286096   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:45:58.286096   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:45:58.290980   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:45:58.291525   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:45:58.291525   18892 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0510 14:45:58.476734   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0510 14:45:58.476734   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:45:59.259459   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:45:59.259459   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:45:59.259459   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:00.680768   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:00.681792   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:00.695055   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:46:00.696194   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:46:00.696194   18892 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0510 14:46:00.937770   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0510 14:46:00.937770   18892 buildroot.go:172] set auth options {CertDir:C:\Users\HP\.minikube CaCertPath:C:\Users\HP\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\HP\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\HP\.minikube\machines\server.pem ServerKeyPath:C:\Users\HP\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\HP\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\HP\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\HP\.minikube}
I0510 14:46:00.937770   18892 buildroot.go:174] setting up certificates
I0510 14:46:00.937770   18892 provision.go:84] configureAuth start
I0510 14:46:00.937770   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:02.203151   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:02.203151   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:02.203151   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:04.989547   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:04.989547   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:04.989547   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:06.605023   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:06.605023   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:06.605023   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:08.850820   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:08.850820   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:08.861322   18892 provision.go:143] copyHostCerts
I0510 14:46:08.861862   18892 exec_runner.go:144] found C:\Users\HP\.minikube/cert.pem, removing ...
I0510 14:46:08.861862   18892 exec_runner.go:203] rm: C:\Users\HP\.minikube\cert.pem
I0510 14:46:08.861862   18892 exec_runner.go:151] cp: C:\Users\HP\.minikube\certs\cert.pem --> C:\Users\HP\.minikube/cert.pem (1111 bytes)
I0510 14:46:08.863031   18892 exec_runner.go:144] found C:\Users\HP\.minikube/key.pem, removing ...
I0510 14:46:08.863031   18892 exec_runner.go:203] rm: C:\Users\HP\.minikube\key.pem
I0510 14:46:08.863031   18892 exec_runner.go:151] cp: C:\Users\HP\.minikube\certs\key.pem --> C:\Users\HP\.minikube/key.pem (1675 bytes)
I0510 14:46:08.864100   18892 exec_runner.go:144] found C:\Users\HP\.minikube/ca.pem, removing ...
I0510 14:46:08.864100   18892 exec_runner.go:203] rm: C:\Users\HP\.minikube\ca.pem
I0510 14:46:08.864790   18892 exec_runner.go:151] cp: C:\Users\HP\.minikube\certs\ca.pem --> C:\Users\HP\.minikube/ca.pem (1066 bytes)
I0510 14:46:08.865836   18892 provision.go:117] generating server cert: C:\Users\HP\.minikube\machines\server.pem ca-key=C:\Users\HP\.minikube\certs\ca.pem private-key=C:\Users\HP\.minikube\certs\ca-key.pem org=HP.minikube san=[127.0.0.1 172.21.205.249 localhost minikube]
I0510 14:46:09.317416   18892 provision.go:177] copyRemoteCerts
I0510 14:46:09.339542   18892 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0510 14:46:09.339542   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:10.313479   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:10.313479   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:10.313479   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:12.217369   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:12.217369   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:12.217369   18892 sshutil.go:53] new ssh client: &{IP:172.21.205.249 Port:22 SSHKeyPath:C:\Users\HP\.minikube\machines\minikube\id_rsa Username:docker}
I0510 14:46:12.335602   18892 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (2.9960602s)
I0510 14:46:12.336143   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0510 14:46:12.396893   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1066 bytes)
I0510 14:46:12.455566   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\machines\server.pem --> /etc/docker/server.pem (1168 bytes)
I0510 14:46:12.511777   18892 provision.go:87] duration metric: took 11.5740066s to configureAuth
I0510 14:46:12.511777   18892 buildroot.go:189] setting minikube options for container-runtime
I0510 14:46:12.512316   18892 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0510 14:46:12.512316   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:13.522964   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:13.522964   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:13.522964   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:15.195420   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:15.195420   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:15.200983   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:46:15.201021   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:46:15.201021   18892 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0510 14:46:15.344457   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0510 14:46:15.344457   18892 buildroot.go:70] root file system type: tmpfs
I0510 14:46:15.345011   18892 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0510 14:46:15.345011   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:16.228897   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:16.228897   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:16.228897   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:17.364423   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:17.364423   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:17.369569   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:46:17.370148   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:46:17.370148   18892 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0510 14:46:17.565712   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0510 14:46:17.565712   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:18.339284   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:18.339284   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:18.339284   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:19.497398   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:19.497398   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:19.502131   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:46:19.502131   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:46:19.502131   18892 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0510 14:46:19.665894   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0510 14:46:19.665894   18892 machine.go:97] duration metric: took 26.2736823s to provisionDockerMachine
I0510 14:46:19.665894   18892 start.go:293] postStartSetup for "minikube" (driver="hyperv")
I0510 14:46:19.665894   18892 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0510 14:46:19.682571   18892 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0510 14:46:19.682571   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:20.524932   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:20.524932   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:20.524962   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:22.267027   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:22.267027   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:22.267027   18892 sshutil.go:53] new ssh client: &{IP:172.21.205.249 Port:22 SSHKeyPath:C:\Users\HP\.minikube\machines\minikube\id_rsa Username:docker}
I0510 14:46:22.380108   18892 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (2.697354s)
I0510 14:46:22.399266   18892 ssh_runner.go:195] Run: cat /etc/os-release
I0510 14:46:22.407203   18892 info.go:137] Remote host: Buildroot 2023.02.9
I0510 14:46:22.407203   18892 filesync.go:126] Scanning C:\Users\HP\.minikube\addons for local assets ...
I0510 14:46:22.407751   18892 filesync.go:126] Scanning C:\Users\HP\.minikube\files for local assets ...
I0510 14:46:22.408351   18892 start.go:296] duration metric: took 2.7424573s for postStartSetup
I0510 14:46:22.408351   18892 fix.go:56] duration metric: took 29.9429562s for fixHost
I0510 14:46:22.408351   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:23.364531   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:23.364531   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:23.364531   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:24.969268   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:24.969268   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:24.975869   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:46:24.976542   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:46:24.976542   18892 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0510 14:46:25.207493   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: 1715345185.990565589

I0510 14:46:25.207493   18892 fix.go:216] guest clock: 1715345185.990565589
I0510 14:46:25.207493   18892 fix.go:229] Guest: 2024-05-10 14:46:25.990565589 +0200 CEST Remote: 2024-05-10 14:46:22.4083513 +0200 CEST m=+32.475885401 (delta=3.582214289s)
I0510 14:46:25.208145   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:26.358814   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:26.358814   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:26.358814   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:27.607799   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:27.608044   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:27.613167   18892 main.go:141] libmachine: Using SSH client type: native
I0510 14:46:27.613167   18892 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa1c0] 0x4acda0 <nil>  [] 0s} 172.21.205.249 22 <nil> <nil>}
I0510 14:46:27.613167   18892 main.go:141] libmachine: About to run SSH command:
sudo date -s @1715345185
I0510 14:46:27.764412   18892 main.go:141] libmachine: SSH cmd err, output: <nil>: Fri May 10 12:46:25 UTC 2024

I0510 14:46:27.764412   18892 fix.go:236] clock set: Fri May 10 12:46:25 UTC 2024
 (err=<nil>)
I0510 14:46:27.764412   18892 start.go:83] releasing machines lock for "minikube", held for 35.2995246s
I0510 14:46:27.764412   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:28.666964   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:28.666964   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:28.666964   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:30.033674   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:30.033674   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:30.040988   18892 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0510 14:46:30.042755   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:30.077270   18892 ssh_runner.go:195] Run: cat /version.json
I0510 14:46:30.077270   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:46:31.534646   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:31.535658   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:31.535658   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:31.546505   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:46:31.546505   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:31.550404   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:46:34.180294   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:34.180294   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:34.180294   18892 sshutil.go:53] new ssh client: &{IP:172.21.205.249 Port:22 SSHKeyPath:C:\Users\HP\.minikube\machines\minikube\id_rsa Username:docker}
I0510 14:46:34.391639   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:46:34.391639   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:46:34.391957   18892 sshutil.go:53] new ssh client: &{IP:172.21.205.249 Port:22 SSHKeyPath:C:\Users\HP\.minikube\machines\minikube\id_rsa Username:docker}
I0510 14:46:36.552138   18892 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (6.5111494s)
I0510 14:46:36.552138   18892 ssh_runner.go:235] Completed: cat /version.json: (6.4748678s)
W0510 14:46:36.563550   18892 start.go:860] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2001 milliseconds
W0510 14:46:36.563644   18892 out.go:239] ! Ce VM rencontre des difficultés pour accéder à https://registry.k8s.io
W0510 14:46:36.564212   18892 out.go:239] * Pour extraire de nouvelles images externes, vous devrez peut-être configurer un proxy : https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0510 14:46:36.613367   18892 ssh_runner.go:195] Run: systemctl --version
I0510 14:46:36.692093   18892 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0510 14:46:36.722303   18892 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0510 14:46:36.743103   18892 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0510 14:46:36.760245   18892 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0510 14:46:36.760245   18892 start.go:494] detecting cgroup driver to use...
I0510 14:46:36.772511   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0510 14:46:36.818822   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0510 14:46:36.851114   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0510 14:46:36.869751   18892 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0510 14:46:36.883787   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0510 14:46:36.916239   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0510 14:46:36.950776   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0510 14:46:36.986810   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0510 14:46:37.030368   18892 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0510 14:46:37.064733   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0510 14:46:37.105342   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0510 14:46:37.140509   18892 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0510 14:46:37.174126   18892 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0510 14:46:37.208376   18892 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0510 14:46:37.241486   18892 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0510 14:46:37.635707   18892 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0510 14:46:37.676620   18892 start.go:494] detecting cgroup driver to use...
I0510 14:46:37.704183   18892 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0510 14:46:37.796997   18892 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0510 14:46:37.917176   18892 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0510 14:46:38.023194   18892 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0510 14:46:38.086467   18892 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0510 14:46:38.120123   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0510 14:46:38.202725   18892 ssh_runner.go:195] Run: which cri-dockerd
I0510 14:46:38.231319   18892 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0510 14:46:38.254976   18892 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0510 14:46:38.323378   18892 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0510 14:46:38.946899   18892 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0510 14:46:39.484382   18892 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0510 14:46:39.484382   18892 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0510 14:46:39.558314   18892 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0510 14:46:40.025555   18892 ssh_runner.go:195] Run: sudo systemctl restart docker
I0510 14:46:53.654325   18892 ssh_runner.go:235] Completed: sudo systemctl restart docker: (13.6287466s)
I0510 14:46:53.680404   18892 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0510 14:46:53.797257   18892 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0510 14:46:53.976267   18892 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0510 14:46:54.073379   18892 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0510 14:46:54.728020   18892 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0510 14:46:55.027197   18892 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0510 14:46:55.297453   18892 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0510 14:46:55.345807   18892 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0510 14:46:55.383251   18892 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0510 14:46:55.704675   18892 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0510 14:46:55.883252   18892 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0510 14:46:55.912457   18892 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0510 14:46:55.921998   18892 start.go:562] Will wait 60s for crictl version
I0510 14:46:55.939279   18892 ssh_runner.go:195] Run: which crictl
I0510 14:46:55.962101   18892 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0510 14:46:56.078072   18892 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0510 14:46:56.090779   18892 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0510 14:46:56.133997   18892 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0510 14:46:56.184376   18892 out.go:204] * Préparation de Kubernetes v1.30.0 sur Docker 26.0.1...
I0510 14:46:56.186144   18892 ip.go:172] getIPForInterface: searching for "vEthernet (Default Switch)"
I0510 14:46:56.211366   18892 ip.go:186] "Ethernet" does not match prefix "vEthernet (Default Switch)"
I0510 14:46:56.211366   18892 ip.go:181] found prefix matching interface for "vEthernet (Default Switch)": "vEthernet (Default Switch)"
I0510 14:46:56.211366   18892 ip.go:207] Found interface: {Index:26 MTU:1500 Name:vEthernet (Default Switch) HardwareAddr:00:15:5d:67:01:69 Flags:up|broadcast|multicast|running}
I0510 14:46:56.237205   18892 ip.go:210] interface addr: fe80::6634:ae09:4d7d:a623/64
I0510 14:46:56.237205   18892 ip.go:210] interface addr: 172.21.192.1/20
I0510 14:46:56.261385   18892 ssh_runner.go:195] Run: grep 172.21.192.1	host.minikube.internal$ /etc/hosts
I0510 14:46:56.274679   18892 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.21.205.249 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HP:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0510 14:46:56.274679   18892 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0510 14:46:56.286377   18892 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0510 14:46:56.336371   18892 docker.go:685] Got preloaded images: -- stdout --
sopd/mysql_img:latest
sopd/mon_php_img:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
php:7.4-apache
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0510 14:46:56.336371   18892 docker.go:615] Images already preloaded, skipping extraction
I0510 14:46:56.348879   18892 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0510 14:46:56.394031   18892 docker.go:685] Got preloaded images: -- stdout --
sopd/mysql_img:latest
sopd/mon_php_img:latest
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
php:7.4-apache
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0510 14:46:56.394031   18892 cache_images.go:84] Images are preloaded, skipping loading
I0510 14:46:56.394031   18892 kubeadm.go:928] updating node { 172.21.205.249 8443 v1.30.0 docker true true} ...
I0510 14:46:56.395274   18892 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.21.205.249

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0510 14:46:56.407908   18892 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0510 14:46:56.472482   18892 cni.go:84] Creating CNI manager for ""
I0510 14:46:56.472482   18892 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0510 14:46:56.472751   18892 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0510 14:46:56.472751   18892 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:172.21.205.249 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "172.21.205.249"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:172.21.205.249 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0510 14:46:56.472751   18892 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.21.205.249
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 172.21.205.249
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "172.21.205.249"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0510 14:46:56.499143   18892 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0510 14:46:56.519076   18892 binaries.go:44] Found k8s binaries, skipping transfer
I0510 14:46:56.534193   18892 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0510 14:46:56.555192   18892 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (309 bytes)
I0510 14:46:56.599894   18892 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0510 14:46:56.631644   18892 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2156 bytes)
I0510 14:46:56.676970   18892 ssh_runner.go:195] Run: grep 172.21.205.249	control-plane.minikube.internal$ /etc/hosts
I0510 14:46:56.697566   18892 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0510 14:46:57.182540   18892 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0510 14:46:57.293682   18892 certs.go:68] Setting up C:\Users\HP\.minikube\profiles\minikube for IP: 172.21.205.249
I0510 14:46:57.293682   18892 certs.go:194] generating shared ca certs ...
I0510 14:46:57.293682   18892 certs.go:226] acquiring lock for ca certs: {Name:mk783baa358a08fc5f0ece37ea9c0b00cd983e08 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0510 14:46:57.294237   18892 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\HP\.minikube\ca.key
I0510 14:46:57.295555   18892 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\HP\.minikube\proxy-client-ca.key
I0510 14:46:57.296118   18892 certs.go:256] generating profile certs ...
I0510 14:46:57.297542   18892 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\HP\.minikube\profiles\minikube\client.key
I0510 14:46:57.298708   18892 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\HP\.minikube\profiles\minikube\apiserver.key.467f03b2
I0510 14:46:57.300167   18892 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\HP\.minikube\profiles\minikube\proxy-client.key
I0510 14:46:57.302448   18892 certs.go:484] found cert: C:\Users\HP\.minikube\certs\ca-key.pem (1675 bytes)
I0510 14:46:57.303003   18892 certs.go:484] found cert: C:\Users\HP\.minikube\certs\ca.pem (1066 bytes)
I0510 14:46:57.303611   18892 certs.go:484] found cert: C:\Users\HP\.minikube\certs\cert.pem (1111 bytes)
I0510 14:46:57.303611   18892 certs.go:484] found cert: C:\Users\HP\.minikube\certs\key.pem (1675 bytes)
I0510 14:46:57.307832   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0510 14:46:57.510115   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0510 14:46:57.687227   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0510 14:46:57.894963   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0510 14:46:58.107243   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0510 14:46:58.492861   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0510 14:46:58.674189   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0510 14:46:58.939440   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0510 14:46:59.103001   18892 ssh_runner.go:362] scp C:\Users\HP\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0510 14:46:59.505606   18892 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0510 14:46:59.625010   18892 ssh_runner.go:195] Run: openssl version
I0510 14:46:59.692996   18892 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0510 14:46:59.861155   18892 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0510 14:46:59.889093   18892 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  9 15:50 /usr/share/ca-certificates/minikubeCA.pem
I0510 14:46:59.918570   18892 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0510 14:46:59.992593   18892 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0510 14:47:00.101391   18892 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0510 14:47:00.153747   18892 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0510 14:47:00.238023   18892 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0510 14:47:00.301510   18892 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0510 14:47:00.344078   18892 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0510 14:47:00.432579   18892 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0510 14:47:00.475095   18892 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0510 14:47:00.519975   18892 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.33.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.21.205.249 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HP:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0510 14:47:00.532900   18892 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0510 14:47:00.640122   18892 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0510 14:47:00.749907   18892 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0510 14:47:00.749907   18892 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0510 14:47:00.749907   18892 kubeadm.go:587] restartPrimaryControlPlane start ...
I0510 14:47:00.803298   18892 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0510 14:47:00.921402   18892 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0510 14:47:00.927580   18892 kubeconfig.go:125] found "minikube" server: "https://172.21.205.249:8443"
I0510 14:47:00.982233   18892 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0510 14:47:01.086578   18892 kubeadm.go:624] The running cluster does not require reconfiguration: 172.21.205.249
I0510 14:47:01.086578   18892 kubeadm.go:1154] stopping kube-system containers ...
I0510 14:47:01.108934   18892 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0510 14:47:01.863354   18892 docker.go:483] Stopping containers: [53f7ae0b3deb b2944bdb3ce6 9bd3c6b2f9fc 491815b8ba93 50bbc458c8e8 7d2be3b9b19b 489373db9fc9 c148a5f75c25 a5442c910dbb 687403492fc0 6e227e1f10f1 5e92b274da09 5f4cd8fecf90 ab3b29626cfe e73fc2cc077e edc965505d7e 906fdf76bbd9 afb515679979 bd03c461884e 0ce651e0385b 87682406b183 db0829281470 97180916f696 085c9354fb12 8cf4ffe6070e 16ebdf6f2cb5 3cb03ea0c295 a49f113d1188 88b666dffb0f aa1b612302f5 9bd21dbbac5d 30f7e9022aab f494b759fd9a]
I0510 14:47:01.887482   18892 ssh_runner.go:195] Run: docker stop 53f7ae0b3deb b2944bdb3ce6 9bd3c6b2f9fc 491815b8ba93 50bbc458c8e8 7d2be3b9b19b 489373db9fc9 c148a5f75c25 a5442c910dbb 687403492fc0 6e227e1f10f1 5e92b274da09 5f4cd8fecf90 ab3b29626cfe e73fc2cc077e edc965505d7e 906fdf76bbd9 afb515679979 bd03c461884e 0ce651e0385b 87682406b183 db0829281470 97180916f696 085c9354fb12 8cf4ffe6070e 16ebdf6f2cb5 3cb03ea0c295 a49f113d1188 88b666dffb0f aa1b612302f5 9bd21dbbac5d 30f7e9022aab f494b759fd9a
I0510 14:47:07.013493   18892 ssh_runner.go:235] Completed: docker stop 53f7ae0b3deb b2944bdb3ce6 9bd3c6b2f9fc 491815b8ba93 50bbc458c8e8 7d2be3b9b19b 489373db9fc9 c148a5f75c25 a5442c910dbb 687403492fc0 6e227e1f10f1 5e92b274da09 5f4cd8fecf90 ab3b29626cfe e73fc2cc077e edc965505d7e 906fdf76bbd9 afb515679979 bd03c461884e 0ce651e0385b 87682406b183 db0829281470 97180916f696 085c9354fb12 8cf4ffe6070e 16ebdf6f2cb5 3cb03ea0c295 a49f113d1188 88b666dffb0f aa1b612302f5 9bd21dbbac5d 30f7e9022aab f494b759fd9a: (5.1260105s)
I0510 14:47:07.063872   18892 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0510 14:47:07.464310   18892 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0510 14:47:07.596649   18892 kubeadm.go:156] found existing configuration files:
-rw------- 1 root root 5647 May  9 15:50 /etc/kubernetes/admin.conf
-rw------- 1 root root 5654 May 10 11:18 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 May  9 15:50 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5606 May 10 11:18 /etc/kubernetes/scheduler.conf

I0510 14:47:07.646293   18892 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0510 14:47:07.804337   18892 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0510 14:47:07.886095   18892 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0510 14:47:07.928674   18892 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0510 14:47:07.964174   18892 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0510 14:47:08.060249   18892 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0510 14:47:08.103696   18892 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0510 14:47:08.138825   18892 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0510 14:47:08.194084   18892 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0510 14:47:08.220548   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0510 14:47:08.338806   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0510 14:47:10.511228   18892 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.1724218s)
I0510 14:47:10.511228   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0510 14:47:11.677513   18892 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml": (1.1662844s)
I0510 14:47:11.677513   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0510 14:47:12.041719   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0510 14:47:12.427973   18892 api_server.go:52] waiting for apiserver process to appear ...
I0510 14:47:12.463295   18892 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0510 14:47:12.977491   18892 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0510 14:47:13.475833   18892 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0510 14:47:13.971317   18892 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0510 14:47:14.468838   18892 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0510 14:47:14.717559   18892 api_server.go:72] duration metric: took 2.2895867s to wait for apiserver process to appear ...
I0510 14:47:14.717559   18892 api_server.go:88] waiting for apiserver healthz status ...
I0510 14:47:14.717559   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:19.739139   18892 api_server.go:269] stopped: https://172.21.205.249:8443/healthz: Get "https://172.21.205.249:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0510 14:47:19.739139   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:22.307974   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0510 14:47:22.307974   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0510 14:47:22.308490   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:22.355053   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0510 14:47:22.355053   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0510 14:47:22.723768   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:22.733978   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:22.733978   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:23.220629   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:23.244308   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:23.244308   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:23.723034   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:23.788313   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:23.788816   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:24.229232   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:24.297618   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:24.297647   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:24.727544   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:24.779432   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:24.780130   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:25.222226   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:25.282212   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:25.282212   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:25.718872   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:25.757260   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:25.757260   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:26.217807   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:26.248241   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0510 14:47:26.248300   18892 api_server.go:103] status: https://172.21.205.249:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0510 14:47:26.730710   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:26.918426   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 200:
ok
I0510 14:47:27.146126   18892 api_server.go:141] control plane version: v1.30.0
I0510 14:47:27.146126   18892 api_server.go:131] duration metric: took 12.4285661s to wait for apiserver health ...
I0510 14:47:27.146680   18892 cni.go:84] Creating CNI manager for ""
I0510 14:47:27.146742   18892 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0510 14:47:27.149321   18892 out.go:177] * Configuration de bridge CNI (Container Networking Interface)...
I0510 14:47:27.209646   18892 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0510 14:47:27.406257   18892 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0510 14:47:27.598182   18892 system_pods.go:43] waiting for kube-system pods to appear ...
I0510 14:47:27.731467   18892 system_pods.go:59] 7 kube-system pods found
I0510 14:47:27.732488   18892 system_pods.go:61] "coredns-7db6d8ff4d-8dzp8" [e5a438ac-4572-4576-a8ba-6aa0f16888c4] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0510 14:47:27.732488   18892 system_pods.go:61] "etcd-minikube" [4a1763e6-368e-470e-b9b8-56682ff9f924] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0510 14:47:27.732488   18892 system_pods.go:61] "kube-apiserver-minikube" [d19840b2-2eee-4b15-b346-0df8a479567b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0510 14:47:27.732488   18892 system_pods.go:61] "kube-controller-manager-minikube" [6448e79e-b141-45ca-b6d3-e417de0e2153] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0510 14:47:27.732488   18892 system_pods.go:61] "kube-proxy-46fmk" [0287f939-4d53-48e3-8da9-d92974efe6d7] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0510 14:47:27.732488   18892 system_pods.go:61] "kube-scheduler-minikube" [0a4096dc-de8a-4897-8912-cb0f54b54fbe] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0510 14:47:27.732488   18892 system_pods.go:61] "storage-provisioner" [77c85896-dac8-498b-bd7a-d8e20f33b09d] Running
I0510 14:47:27.732488   18892 system_pods.go:74] duration metric: took 134.3063ms to wait for pod list to return data ...
I0510 14:47:27.732488   18892 node_conditions.go:102] verifying NodePressure condition ...
I0510 14:47:27.786983   18892 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0510 14:47:27.786983   18892 node_conditions.go:123] node cpu capacity is 2
I0510 14:47:27.786983   18892 node_conditions.go:105] duration metric: took 54.4947ms to run NodePressure ...
I0510 14:47:27.786983   18892 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0510 14:47:29.922990   18892 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.1358967s)
I0510 14:47:29.922990   18892 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0510 14:47:29.976646   18892 ops.go:34] apiserver oom_adj: -16
I0510 14:47:29.976646   18892 kubeadm.go:591] duration metric: took 29.2267386s to restartPrimaryControlPlane
I0510 14:47:29.976646   18892 kubeadm.go:393] duration metric: took 29.4566709s to StartCluster
I0510 14:47:29.976646   18892 settings.go:142] acquiring lock: {Name:mk0d2b59703643596641074377d4ead40a4834ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0510 14:47:29.976646   18892 settings.go:150] Updating kubeconfig:  C:\Users\HP\.kube\config
I0510 14:47:29.979800   18892 lock.go:35] WriteFile acquiring C:\Users\HP\.kube\config: {Name:mkd67ce5dd51c0f5cd2031e54eb964170051b039 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0510 14:47:29.983818   18892 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0510 14:47:29.983818   18892 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0510 14:47:29.983818   18892 addons.go:69] Setting dashboard=true in profile "minikube"
I0510 14:47:29.984423   18892 start.go:234] Will wait 6m0s for node &{Name: IP:172.21.205.249 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0510 14:47:29.986939   18892 out.go:177] * Vérification des composants Kubernetes...
I0510 14:47:29.984423   18892 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0510 14:47:29.984423   18892 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0510 14:47:29.984423   18892 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0510 14:47:29.984423   18892 addons.go:234] Setting addon dashboard=true in "minikube"
I0510 14:47:29.988286   18892 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0510 14:47:29.991209   18892 addons.go:243] addon storage-provisioner should already be in state true
I0510 14:47:29.992208   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
W0510 14:47:29.994207   18892 addons.go:243] addon dashboard should already be in state true
I0510 14:47:29.996991   18892 host.go:66] Checking if "minikube" exists ...
I0510 14:47:29.998992   18892 host.go:66] Checking if "minikube" exists ...
I0510 14:47:30.008091   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:47:30.036593   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:47:30.117146   18892 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0510 14:47:34.897969   18892 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (4.7808228s)
I0510 14:47:34.976194   18892 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0510 14:47:35.495387   18892 api_server.go:52] waiting for apiserver process to appear ...
I0510 14:47:35.597100   18892 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0510 14:47:35.734069   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:47:35.734069   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:35.784776   18892 out.go:177]   - Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I0510 14:47:35.796919   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:47:35.796919   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:35.810081   18892 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0510 14:47:35.810081   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0510 14:47:35.810081   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:47:35.806060   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:47:35.815515   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:35.841266   18892 out.go:177]   - Utilisation de l'image docker.io/kubernetesui/dashboard:v2.7.0
I0510 14:47:35.839316   18892 api_server.go:72] duration metric: took 5.8548931s to wait for apiserver process to appear ...
I0510 14:47:35.846348   18892 api_server.go:88] waiting for apiserver healthz status ...
I0510 14:47:35.846348   18892 api_server.go:253] Checking apiserver healthz at https://172.21.205.249:8443/healthz ...
I0510 14:47:35.861955   18892 out.go:177]   - Utilisation de l'image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0510 14:47:35.876303   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0510 14:47:35.876303   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0510 14:47:35.876303   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:47:35.924362   18892 api_server.go:279] https://172.21.205.249:8443/healthz returned 200:
ok
I0510 14:47:35.945280   18892 api_server.go:141] control plane version: v1.30.0
I0510 14:47:35.945280   18892 api_server.go:131] duration metric: took 98.9323ms to wait for apiserver health ...
I0510 14:47:35.945280   18892 system_pods.go:43] waiting for kube-system pods to appear ...
I0510 14:47:36.010888   18892 system_pods.go:59] 7 kube-system pods found
I0510 14:47:36.010888   18892 system_pods.go:61] "coredns-7db6d8ff4d-8dzp8" [e5a438ac-4572-4576-a8ba-6aa0f16888c4] Running
I0510 14:47:36.010888   18892 system_pods.go:61] "etcd-minikube" [4a1763e6-368e-470e-b9b8-56682ff9f924] Running
I0510 14:47:36.010888   18892 system_pods.go:61] "kube-apiserver-minikube" [d19840b2-2eee-4b15-b346-0df8a479567b] Running
I0510 14:47:36.010888   18892 system_pods.go:61] "kube-controller-manager-minikube" [6448e79e-b141-45ca-b6d3-e417de0e2153] Running
I0510 14:47:36.010888   18892 system_pods.go:61] "kube-proxy-46fmk" [0287f939-4d53-48e3-8da9-d92974efe6d7] Running
I0510 14:47:36.010888   18892 system_pods.go:61] "kube-scheduler-minikube" [0a4096dc-de8a-4897-8912-cb0f54b54fbe] Running
I0510 14:47:36.010888   18892 system_pods.go:61] "storage-provisioner" [77c85896-dac8-498b-bd7a-d8e20f33b09d] Running
I0510 14:47:36.010888   18892 system_pods.go:74] duration metric: took 65.6074ms to wait for pod list to return data ...
I0510 14:47:36.010888   18892 kubeadm.go:576] duration metric: took 6.0264652s to wait for: map[apiserver:true system_pods:true]
I0510 14:47:36.010888   18892 node_conditions.go:102] verifying NodePressure condition ...
I0510 14:47:36.041868   18892 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0510 14:47:36.041868   18892 node_conditions.go:123] node cpu capacity is 2
I0510 14:47:36.041868   18892 node_conditions.go:105] duration metric: took 30.9802ms to run NodePressure ...
I0510 14:47:36.041868   18892 start.go:240] waiting for startup goroutines ...
I0510 14:47:38.481767   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:47:38.481767   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:38.481767   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:47:38.499459   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:47:38.499459   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:38.499459   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:47:40.383333   18892 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0510 14:47:40.383333   18892 addons.go:243] addon default-storageclass should already be in state true
I0510 14:47:40.384333   18892 host.go:66] Checking if "minikube" exists ...
I0510 14:47:40.387716   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:47:41.773225   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:47:41.773225   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:41.773225   18892 sshutil.go:53] new ssh client: &{IP:172.21.205.249 Port:22 SSHKeyPath:C:\Users\HP\.minikube\machines\minikube\id_rsa Username:docker}
I0510 14:47:42.085038   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:47:42.085038   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:42.085038   18892 sshutil.go:53] new ssh client: &{IP:172.21.205.249 Port:22 SSHKeyPath:C:\Users\HP\.minikube\machines\minikube\id_rsa Username:docker}
I0510 14:47:42.285907   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:47:42.285907   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:42.285907   18892 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0510 14:47:42.285907   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0510 14:47:42.285907   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0510 14:47:42.481763   18892 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0510 14:47:42.960000   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0510 14:47:42.960000   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0510 14:47:43.196009   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0510 14:47:43.196009   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0510 14:47:43.578523   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0510 14:47:43.578523   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0510 14:47:43.660967   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0510 14:47:43.660967   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0510 14:47:43.739360   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-role.yaml
I0510 14:47:43.739360   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0510 14:47:43.832634   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0510 14:47:43.832634   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0510 14:47:43.839054   18892 main.go:141] libmachine: [stdout =====>] : Running

I0510 14:47:43.839054   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:43.839054   18892 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0510 14:47:43.931338   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0510 14:47:43.931338   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0510 14:47:44.081576   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0510 14:47:44.081576   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0510 14:47:44.174576   18892 addons.go:426] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0510 14:47:44.174576   18892 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0510 14:47:44.330367   18892 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0510 14:47:46.020783   18892 main.go:141] libmachine: [stdout =====>] : 172.21.205.249

I0510 14:47:46.020783   18892 main.go:141] libmachine: [stderr =====>] : 
I0510 14:47:46.021317   18892 sshutil.go:53] new ssh client: &{IP:172.21.205.249 Port:22 SSHKeyPath:C:\Users\HP\.minikube\machines\minikube\id_rsa Username:docker}
I0510 14:47:46.713295   18892 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.2315317s)
I0510 14:47:46.785791   18892 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0510 14:47:50.038051   18892 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.2520095s)
I0510 14:47:50.038051   18892 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (5.7076832s)
I0510 14:47:50.048149   18892 out.go:177] * Certaines fonctionnalités du tableau de bord nécessitent le module complémentaire metrics-server. Pour activer toutes les fonctionnalités, veuillez exécuter :

	minikube addons enable metrics-server

I0510 14:47:50.061340   18892 out.go:177] * Modules activés: storage-provisioner, dashboard, default-storageclass
I0510 14:47:50.065047   18892 addons.go:505] duration metric: took 20.0812293s for enable addons: enabled=[storage-provisioner dashboard default-storageclass]
I0510 14:47:50.065047   18892 start.go:245] waiting for cluster config update ...
I0510 14:47:50.065047   18892 start.go:254] writing updated cluster config ...
I0510 14:47:50.101309   18892 ssh_runner.go:195] Run: rm -f paused
I0510 14:47:50.292207   18892 start.go:600] kubectl: 1.29.1, cluster: 1.30.0 (minor skew: 1)
I0510 14:47:50.294605   18892 out.go:177] * Terminé ! kubectl est maintenant configuré pour utiliser "minikube" cluster et espace de noms "default" par défaut.


==> Docker <==
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.364571140Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.364960539Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.365445539Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.427333718Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.578135668Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.578272767Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.578309067Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:15 minikube dockerd[118801]: time="2024-05-10T12:47:15.578553267Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:15 minikube cri-dockerd[119044]: time="2024-05-10T12:47:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/16171385adb63fbba8b48ac62fb2bade05a727fcee7287e6ebb6515f897724c5/resolv.conf as [nameserver 172.21.192.1]"
May 10 12:47:16 minikube cri-dockerd[119044]: time="2024-05-10T12:47:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/224cae5ea4ac11a8adcd9b671497099400465dfc206824ed1b480ce86342c145/resolv.conf as [nameserver 172.21.192.1]"
May 10 12:47:16 minikube cri-dockerd[119044]: time="2024-05-10T12:47:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/534627a8614148248b905864ae1e0ea8b164895a3ef56db06a298b6d1a671dbd/resolv.conf as [nameserver 172.21.192.1]"
May 10 12:47:16 minikube cri-dockerd[119044]: time="2024-05-10T12:47:16Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-8dzp8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9bd3c6b2f9fc67389b221dddb005cdf2a0907e988472024f581f53e9853e5f97\""
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.396679290Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.401770089Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.404863088Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.406251587Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.592888424Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.593018424Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.593095924Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.593358424Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.644075106Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.644831606Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.645178006Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:16 minikube dockerd[118801]: time="2024-05-10T12:47:16.647744105Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:23 minikube cri-dockerd[119044]: time="2024-05-10T12:47:23Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 10 12:47:24 minikube dockerd[118801]: time="2024-05-10T12:47:24.857599838Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:24 minikube dockerd[118801]: time="2024-05-10T12:47:24.857839737Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:24 minikube dockerd[118801]: time="2024-05-10T12:47:24.857880937Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:24 minikube dockerd[118801]: time="2024-05-10T12:47:24.858138237Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.041638972Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.041913672Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.041964671Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.042275071Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.114225045Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.114556145Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.114672445Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.118701644Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.158499730Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.158682829Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.159050729Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.160005529Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.214742509Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.215010709Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.215062609Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube dockerd[118801]: time="2024-05-10T12:47:25.215339009Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:25 minikube cri-dockerd[119044]: time="2024-05-10T12:47:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/58e89b16e4475bbb2e96523355f4bb08a955d089045cc1445911bb58277b1349/resolv.conf as [nameserver 172.21.192.1]"
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.127892480Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.134109678Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.134484178Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.134913178Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:26 minikube cri-dockerd[119044]: time="2024-05-10T12:47:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/df55ba40302f2488a125bfe8828750536322f8d39326a9500861a79a7f3ceabb/resolv.conf as [nameserver 172.21.192.1]"
May 10 12:47:26 minikube cri-dockerd[119044]: time="2024-05-10T12:47:26Z" level=info msg="Stop pulling image sopd/mysql_img:latest: Status: Image is up to date for sopd/mysql_img:latest"
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.813422732Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.817846230Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.817983330Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.818377130Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.965063377Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.965397377Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.965551277Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 10 12:47:26 minikube dockerd[118801]: time="2024-05-10T12:47:26.965954577Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1


==> container status <==
CONTAINER           IMAGE                                                                                    CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
777870f502bfc       cbb01a7bd410d                                                                            11 minutes ago      Running             coredns                     5                   df55ba40302f2       coredns-7db6d8ff4d-8dzp8
dfa386c8ea906       sopd/mysql_img@sha256:ac637fad3b82d8504de3032481ece59c505b265bc4eb471e87402be0df482a45   11 minutes ago      Running             mysql-container             2                   c041c45ff878e       mysql-deployment-7985ff9c55-7bt2g
88e6485218e64       a0bf559e280cf                                                                            11 minutes ago      Running             kube-proxy                  7                   58e89b16e4475       kube-proxy-46fmk
3199012c16203       6e38f40d628db                                                                            11 minutes ago      Running             storage-provisioner         12                  4c55faead7ed4       storage-provisioner
54164f977423f       07655ddf2eebe                                                                            11 minutes ago      Running             kubernetes-dashboard        4                   a97d0426f11e5       kubernetes-dashboard-779776cb65-gksjb
8849d23ef9583       115053965e86b                                                                            11 minutes ago      Running             dashboard-metrics-scraper   3                   bc3b06ad05223       dashboard-metrics-scraper-b5fc48f67-26qqt
ed6bb1150e682       c7aad43836fa5                                                                            11 minutes ago      Running             kube-controller-manager     7                   534627a861414       kube-controller-manager-minikube
05b4a23545c42       259c8277fcbbc                                                                            11 minutes ago      Running             kube-scheduler              7                   224cae5ea4ac1       kube-scheduler-minikube
47309c26ab30d       3861cfcd7c04c                                                                            11 minutes ago      Running             etcd                        7                   16171385adb63       etcd-minikube
1f09b3610644b       c42f13656d0b2                                                                            11 minutes ago      Running             kube-apiserver              7                   0913961499c70       kube-apiserver-minikube
2f147b0485d77       7e84e89fb8c7a                                                                            11 minutes ago      Running             php-container               2                   142a5f3c665ad       php-deployment-5d8b6cd79-9fznt
62f9612d2ab33       3861cfcd7c04c                                                                            11 minutes ago      Created             etcd                        6                   50bbc458c8e8b       etcd-minikube
8c27d495d41df       a0bf559e280cf                                                                            11 minutes ago      Created             kube-proxy                  6                   7d2be3b9b19b3       kube-proxy-46fmk
81544f66137f9       c7aad43836fa5                                                                            11 minutes ago      Created             kube-controller-manager     6                   489373db9fc91       kube-controller-manager-minikube
53f7ae0b3debd       c42f13656d0b2                                                                            11 minutes ago      Created             kube-apiserver              6                   c148a5f75c25e       kube-apiserver-minikube
b2944bdb3ce60       259c8277fcbbc                                                                            11 minutes ago      Exited              kube-scheduler              6                   a5442c910dbbb       kube-scheduler-minikube
491815b8ba938       6e38f40d628db                                                                            11 minutes ago      Exited              storage-provisioner         11                  687403492fc05       storage-provisioner
b1f000b94c2b9       sopd/mysql_img@sha256:ac637fad3b82d8504de3032481ece59c505b265bc4eb471e87402be0df482a45   2 hours ago         Exited              mysql-container             1                   c2df889a44c48       mysql-deployment-7985ff9c55-7bt2g
5e92b274da093       cbb01a7bd410d                                                                            2 hours ago         Exited              coredns                     4                   e73fc2cc077e9       coredns-7db6d8ff4d-8dzp8
ff88b2e23f2eb       07655ddf2eebe                                                                            2 hours ago         Exited              kubernetes-dashboard        3                   e6d537922a714       kubernetes-dashboard-779776cb65-gksjb
e47bf2bbdad5c       7e84e89fb8c7a                                                                            2 hours ago         Exited              php-container               1                   aa4efcf03dbb1       php-deployment-5d8b6cd79-9fznt
83eeb9044aa08       115053965e86b                                                                            2 hours ago         Exited              dashboard-metrics-scraper   2                   6726143888f96       dashboard-metrics-scraper-b5fc48f67-26qqt


==> coredns [5e92b274da09] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 6dac6e834f846355a6dedea9bb48d835ecaf67275ef397241e2151bdb93c93b9570eef4aafa206751aa0f780c5a91826352161c470ac67f779bb18384340a7e8
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:58961 - 6017 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 6.024946118s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:44233->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:46012 - 27938 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 6.00296515s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:46601->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:49778 - 16783 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 4.002514133s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:57565->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:36792 - 60160 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 2.000390389s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:46886->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:40841 - 60038 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 2.002140181s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:34934->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:35858 - 10512 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 2.001470685s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:45411->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:45567 - 11234 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 2.002172581s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:53765->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:51436 - 61986 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 2.001962782s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:46668->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:52140 - 6782 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 2.001206286s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:54102->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:35960 - 40163 "HINFO IN 228982349030674848.1028192047469256782. udp 56 false 512" - - 0 2.001419883s
[ERROR] plugin/errors: 2 228982349030674848.1028192047469256782. HINFO: read udp 10.244.0.21:45737->172.21.192.1:53: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [777870f502bf] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 6dac6e834f846355a6dedea9bb48d835ecaf67275ef397241e2151bdb93c93b9570eef4aafa206751aa0f780c5a91826352161c470ac67f779bb18384340a7e8
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:55895 - 30585 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 6.019912209s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:55612->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:48634 - 40864 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 6.025734596s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:34187->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:56400 - 57328 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 4.024222019s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:55294->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:46220 - 51677 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 2.000590474s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:56103->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:35630 - 9879 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 2.00981336s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:40977->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:53578 - 33905 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 2.00788616s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:46647->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:53420 - 34302 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 2.003994865s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:51462->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:48566 - 64040 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 2.000588668s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:54041->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:42811 - 29374 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 2.001199765s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:44172->172.21.192.1:53: i/o timeout
[INFO] 127.0.0.1:52726 - 26093 "HINFO IN 8559084250026672421.7942022479077459502. udp 57 false 512" - - 0 2.001503764s
[ERROR] plugin/errors: 2 8559084250026672421.7942022479077459502. HINFO: read udp 10.244.0.26:37892->172.21.192.1:53: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_09T17_50_30_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 09 May 2024 15:50:24 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 10 May 2024 12:58:50 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 10 May 2024 12:57:35 +0000   Thu, 09 May 2024 15:50:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 10 May 2024 12:57:35 +0000   Thu, 09 May 2024 15:50:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 10 May 2024 12:57:35 +0000   Thu, 09 May 2024 15:50:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 10 May 2024 12:57:35 +0000   Thu, 09 May 2024 15:50:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.21.205.249
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912872Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912872Ki
  pods:               110
System Info:
  Machine ID:                 913b696417434d309989e0445bd54415
  System UUID:                fd716fc4-97e1-4d42-9a42-15c139d1a4d6
  Boot ID:                    0729e795-424e-4e72-8e8f-b3865c96fe7c
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     mysql-deployment-7985ff9c55-7bt2g            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         110m
  default                     php-deployment-5d8b6cd79-9fznt               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         148m
  kube-system                 coredns-7db6d8ff4d-8dzp8                     100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     21h
  kube-system                 etcd-minikube                                100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         21h
  kube-system                 kube-apiserver-minikube                      250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 kube-controller-manager-minikube             200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 kube-proxy-46fmk                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 kube-scheduler-minikube                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21h
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-26qqt    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-gksjb        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 11m                kube-proxy       
  Normal  Starting                 11m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     11m (x7 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  11m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           11m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May10 08:15] systemd-fstab-generator[57878]: Ignoring "noauto" option for root device
[  +0.788918] systemd-fstab-generator[57914]: Ignoring "noauto" option for root device
[  +0.338263] systemd-fstab-generator[57926]: Ignoring "noauto" option for root device
[  +0.381698] systemd-fstab-generator[57955]: Ignoring "noauto" option for root device
[  +5.449884] kauditd_printk_skb: 98 callbacks suppressed
[  +8.059034] systemd-fstab-generator[58726]: Ignoring "noauto" option for root device
[  +0.253377] systemd-fstab-generator[58738]: Ignoring "noauto" option for root device
[  +0.239237] systemd-fstab-generator[58750]: Ignoring "noauto" option for root device
[  +0.330598] systemd-fstab-generator[58795]: Ignoring "noauto" option for root device
[  +1.022795] systemd-fstab-generator[59018]: Ignoring "noauto" option for root device
[  +0.158120] kauditd_printk_skb: 122 callbacks suppressed
[ +12.105959] kauditd_printk_skb: 134 callbacks suppressed
[  +2.934950] systemd-fstab-generator[60646]: Ignoring "noauto" option for root device
[May10 08:16] kauditd_printk_skb: 49 callbacks suppressed
[  +3.078905] systemd-fstab-generator[61366]: Ignoring "noauto" option for root device
[ +11.314065] kauditd_printk_skb: 53 callbacks suppressed
[May10 10:07] kauditd_printk_skb: 2 callbacks suppressed
[May10 10:16] kauditd_printk_skb: 5 callbacks suppressed
[May10 10:30] kauditd_printk_skb: 2 callbacks suppressed
[  +5.102832] kauditd_printk_skb: 2 callbacks suppressed
[May10 10:33] kauditd_printk_skb: 2 callbacks suppressed
[May10 10:35] kauditd_printk_skb: 2 callbacks suppressed
[May10 11:17] systemd-fstab-generator[97395]: Ignoring "noauto" option for root device
[  +0.285356] kauditd_printk_skb: 9 callbacks suppressed
[  +0.701197] systemd-fstab-generator[97432]: Ignoring "noauto" option for root device
[  +0.524585] systemd-fstab-generator[97445]: Ignoring "noauto" option for root device
[  +0.629281] systemd-fstab-generator[97458]: Ignoring "noauto" option for root device
[May10 11:18] kauditd_printk_skb: 101 callbacks suppressed
[  +8.623352] systemd-fstab-generator[98295]: Ignoring "noauto" option for root device
[  +0.487963] systemd-fstab-generator[98307]: Ignoring "noauto" option for root device
[  +0.369959] systemd-fstab-generator[98319]: Ignoring "noauto" option for root device
[  +0.487814] systemd-fstab-generator[98334]: Ignoring "noauto" option for root device
[  +0.149529] kauditd_printk_skb: 96 callbacks suppressed
[  +0.964437] systemd-fstab-generator[98490]: Ignoring "noauto" option for root device
[  +3.889463] systemd-fstab-generator[98593]: Ignoring "noauto" option for root device
[  +0.148976] kauditd_printk_skb: 38 callbacks suppressed
[  +9.897338] kauditd_printk_skb: 48 callbacks suppressed
[  +4.350705] systemd-fstab-generator[99690]: Ignoring "noauto" option for root device
[  +0.750866] kauditd_printk_skb: 52 callbacks suppressed
[  +5.689188] kauditd_printk_skb: 47 callbacks suppressed
[  +5.637880] kauditd_printk_skb: 14 callbacks suppressed
[May10 12:46] systemd-fstab-generator[118102]: Ignoring "noauto" option for root device
[  +1.151604] systemd-fstab-generator[118139]: Ignoring "noauto" option for root device
[  +0.668760] systemd-fstab-generator[118151]: Ignoring "noauto" option for root device
[  +0.529729] systemd-fstab-generator[118165]: Ignoring "noauto" option for root device
[  +2.934094] kauditd_printk_skb: 97 callbacks suppressed
[ +11.754303] systemd-fstab-generator[118993]: Ignoring "noauto" option for root device
[  +0.443666] systemd-fstab-generator[119006]: Ignoring "noauto" option for root device
[  +0.304053] systemd-fstab-generator[119017]: Ignoring "noauto" option for root device
[  +0.377330] systemd-fstab-generator[119033]: Ignoring "noauto" option for root device
[  +0.168166] kauditd_printk_skb: 96 callbacks suppressed
[  +1.169585] systemd-fstab-generator[119185]: Ignoring "noauto" option for root device
[  +4.556177] kauditd_printk_skb: 69 callbacks suppressed
[May10 12:47] kauditd_printk_skb: 47 callbacks suppressed
[  +4.676669] systemd-fstab-generator[120806]: Ignoring "noauto" option for root device
[  +0.528465] kauditd_printk_skb: 25 callbacks suppressed
[ +13.046455] kauditd_printk_skb: 47 callbacks suppressed
[  +6.506487] systemd-fstab-generator[121894]: Ignoring "noauto" option for root device
[  +3.370608] kauditd_printk_skb: 41 callbacks suppressed
[  +7.260236] kauditd_printk_skb: 12 callbacks suppressed


==> etcd [47309c26ab30] <==
{"level":"info","ts":"2024-05-10T12:47:42.632027Z","caller":"traceutil/trace.go:171","msg":"trace[1897108204] transaction","detail":"{read_only:false; response_revision:27054; number_of_response:1; }","duration":"253.622655ms","start":"2024-05-10T12:47:42.378322Z","end":"2024-05-10T12:47:42.631945Z","steps":["trace[1897108204] 'process raft request'  (duration: 182.672851ms)","trace[1897108204] 'compare'  (duration: 55.048625ms)"],"step_count":2}
{"level":"info","ts":"2024-05-10T12:47:42.632213Z","caller":"traceutil/trace.go:171","msg":"trace[1182244945] transaction","detail":"{read_only:false; response_revision:27055; number_of_response:1; }","duration":"253.609054ms","start":"2024-05-10T12:47:42.378589Z","end":"2024-05-10T12:47:42.632198Z","steps":["trace[1182244945] 'process raft request'  (duration: 237.664076ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:47:42.632455Z","caller":"traceutil/trace.go:171","msg":"trace[1396576073] transaction","detail":"{read_only:false; response_revision:27056; number_of_response:1; }","duration":"253.498854ms","start":"2024-05-10T12:47:42.378888Z","end":"2024-05-10T12:47:42.632387Z","steps":["trace[1396576073] 'process raft request'  (duration: 247.208363ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-10T12:47:42.667193Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"291.419903ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/mysql-deployment\" ","response":"range_response_count:1 size:2860"}
{"level":"warn","ts":"2024-05-10T12:47:42.667441Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"292.591501ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:4959"}
{"level":"warn","ts":"2024-05-10T12:47:42.66758Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"319.575864ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67\" ","response":"range_response_count:1 size:3149"}
{"level":"warn","ts":"2024-05-10T12:47:42.667705Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"319.788564ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/kube-system/coredns-7db6d8ff4d\" ","response":"range_response_count:1 size:3847"}
{"level":"warn","ts":"2024-05-10T12:47:42.667786Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"319.963564ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/php-deployment-5d8b6cd79\" ","response":"range_response_count:1 size:2169"}
{"level":"warn","ts":"2024-05-10T12:47:42.66789Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"320.544563ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/mysql-deployment-7985ff9c55\" ","response":"range_response_count:1 size:2068"}
{"level":"warn","ts":"2024-05-10T12:47:42.703464Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"326.920554ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/kube-system/coredns\" ","response":"range_response_count:1 size:4127"}
{"level":"warn","ts":"2024-05-10T12:47:42.704125Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"327.515953ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:4978"}
{"level":"info","ts":"2024-05-10T12:47:42.720819Z","caller":"traceutil/trace.go:171","msg":"trace[1203936881] range","detail":"{range_begin:/registry/deployments/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:27056; }","duration":"344.32773ms","start":"2024-05-10T12:47:42.376423Z","end":"2024-05-10T12:47:42.72075Z","steps":["trace[1203936881] 'agreement among raft nodes before linearized reading'  (duration: 327.461853ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-10T12:47:42.720952Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.376414Z","time spent":"344.49553ms","remote":"127.0.0.1:39286","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":5000,"request content":"key:\"/registry/deployments/kubernetes-dashboard/dashboard-metrics-scraper\" "}
{"level":"warn","ts":"2024-05-10T12:47:42.727758Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"380.802181ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/kubernetes-dashboard/kubernetes-dashboard-779776cb65\" ","response":"range_response_count:1 size:3187"}
{"level":"info","ts":"2024-05-10T12:47:42.727862Z","caller":"traceutil/trace.go:171","msg":"trace[631503805] range","detail":"{range_begin:/registry/replicasets/kubernetes-dashboard/kubernetes-dashboard-779776cb65; range_end:; response_count:1; response_revision:27056; }","duration":"380.955881ms","start":"2024-05-10T12:47:42.346879Z","end":"2024-05-10T12:47:42.727835Z","steps":["trace[631503805] 'agreement among raft nodes before linearized reading'  (duration: 321.034762ms)","trace[631503805] 'range keys from bolt db'  (duration: 59.783219ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-10T12:47:42.727934Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.346823Z","time spent":"381.092581ms","remote":"127.0.0.1:39302","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":1,"response size":3209,"request content":"key:\"/registry/replicasets/kubernetes-dashboard/kubernetes-dashboard-779776cb65\" "}
{"level":"warn","ts":"2024-05-10T12:47:42.736009Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.378291Z","time spent":"343.332032ms","remote":"127.0.0.1:38984","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":872,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" mod_revision:22731 > success:<request_put:<key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" value_size:789 >> failure:<request_range:<key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" > >"}
{"level":"warn","ts":"2024-05-10T12:47:42.736783Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.378569Z","time spent":"358.032612ms","remote":"127.0.0.1:38984","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":853,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" mod_revision:22728 > success:<request_put:<key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" value_size:775 >> failure:<request_range:<key:\"/registry/services/endpoints/kubernetes-dashboard/kubernetes-dashboard\" > >"}
{"level":"warn","ts":"2024-05-10T12:47:42.737288Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.37887Z","time spent":"358.317411ms","remote":"127.0.0.1:38984","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":782,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/kube-dns\" mod_revision:22750 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/kube-dns\" value_size:725 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/kube-dns\" > >"}
{"level":"info","ts":"2024-05-10T12:47:42.739617Z","caller":"traceutil/trace.go:171","msg":"trace[753331431] range","detail":"{range_begin:/registry/deployments/default/mysql-deployment; range_end:; response_count:1; response_revision:27056; }","duration":"363.784804ms","start":"2024-05-10T12:47:42.375666Z","end":"2024-05-10T12:47:42.739451Z","steps":["trace[753331431] 'agreement among raft nodes before linearized reading'  (duration: 291.361703ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-10T12:47:42.762075Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.375645Z","time spent":"386.388473ms","remote":"127.0.0.1:39286","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":1,"response size":2882,"request content":"key:\"/registry/deployments/default/mysql-deployment\" "}
{"level":"info","ts":"2024-05-10T12:47:42.740067Z","caller":"traceutil/trace.go:171","msg":"trace[760003580] range","detail":"{range_begin:/registry/deployments/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:27056; }","duration":"365.161702ms","start":"2024-05-10T12:47:42.37478Z","end":"2024-05-10T12:47:42.739942Z","steps":["trace[760003580] 'agreement among raft nodes before linearized reading'  (duration: 292.523101ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:47:42.740263Z","caller":"traceutil/trace.go:171","msg":"trace[511787071] range","detail":"{range_begin:/registry/replicasets/kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67; range_end:; response_count:1; response_revision:27056; }","duration":"392.242165ms","start":"2024-05-10T12:47:42.347967Z","end":"2024-05-10T12:47:42.74021Z","steps":["trace[511787071] 'agreement among raft nodes before linearized reading'  (duration: 319.507064ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:47:42.740308Z","caller":"traceutil/trace.go:171","msg":"trace[469534822] range","detail":"{range_begin:/registry/replicasets/kube-system/coredns-7db6d8ff4d; range_end:; response_count:1; response_revision:27056; }","duration":"392.422865ms","start":"2024-05-10T12:47:42.347872Z","end":"2024-05-10T12:47:42.740295Z","steps":["trace[469534822] 'agreement among raft nodes before linearized reading'  (duration: 319.734664ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:47:42.74039Z","caller":"traceutil/trace.go:171","msg":"trace[1548997229] range","detail":"{range_begin:/registry/replicasets/default/php-deployment-5d8b6cd79; range_end:; response_count:1; response_revision:27056; }","duration":"392.594565ms","start":"2024-05-10T12:47:42.347782Z","end":"2024-05-10T12:47:42.740377Z","steps":["trace[1548997229] 'agreement among raft nodes before linearized reading'  (duration: 319.948864ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:47:42.740469Z","caller":"traceutil/trace.go:171","msg":"trace[1145215132] range","detail":"{range_begin:/registry/replicasets/default/mysql-deployment-7985ff9c55; range_end:; response_count:1; response_revision:27056; }","duration":"393.187264ms","start":"2024-05-10T12:47:42.347269Z","end":"2024-05-10T12:47:42.740456Z","steps":["trace[1145215132] 'agreement among raft nodes before linearized reading'  (duration: 320.541463ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:47:42.740567Z","caller":"traceutil/trace.go:171","msg":"trace[510153710] range","detail":"{range_begin:/registry/deployments/kube-system/coredns; range_end:; response_count:1; response_revision:27056; }","duration":"364.025603ms","start":"2024-05-10T12:47:42.376475Z","end":"2024-05-10T12:47:42.7405Z","steps":["trace[510153710] 'agreement among raft nodes before linearized reading'  (duration: 256.013251ms)","trace[510153710] 'get authentication metadata'  (duration: 70.457204ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-10T12:47:42.77388Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.376462Z","time spent":"397.302458ms","remote":"127.0.0.1:39286","response type":"/etcdserverpb.KV/Range","request count":0,"request size":43,"response count":1,"response size":4149,"request content":"key:\"/registry/deployments/kube-system/coredns\" "}
{"level":"warn","ts":"2024-05-10T12:47:42.774579Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.373483Z","time spent":"401.015553ms","remote":"127.0.0.1:39286","response type":"/etcdserverpb.KV/Range","request count":0,"request size":65,"response count":1,"response size":4981,"request content":"key:\"/registry/deployments/kubernetes-dashboard/kubernetes-dashboard\" "}
{"level":"warn","ts":"2024-05-10T12:47:42.775543Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.347958Z","time spent":"427.491117ms","remote":"127.0.0.1:39302","response type":"/etcdserverpb.KV/Range","request count":0,"request size":80,"response count":1,"response size":3171,"request content":"key:\"/registry/replicasets/kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67\" "}
{"level":"warn","ts":"2024-05-10T12:47:42.782565Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.347862Z","time spent":"434.595107ms","remote":"127.0.0.1:39302","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":1,"response size":3869,"request content":"key:\"/registry/replicasets/kube-system/coredns-7db6d8ff4d\" "}
{"level":"warn","ts":"2024-05-10T12:47:42.783151Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.347768Z","time spent":"435.318106ms","remote":"127.0.0.1:39302","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":1,"response size":2191,"request content":"key:\"/registry/replicasets/default/php-deployment-5d8b6cd79\" "}
{"level":"warn","ts":"2024-05-10T12:47:42.787568Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:47:42.347253Z","time spent":"440.229899ms","remote":"127.0.0.1:39302","response type":"/etcdserverpb.KV/Range","request count":0,"request size":59,"response count":1,"response size":2090,"request content":"key:\"/registry/replicasets/default/mysql-deployment-7985ff9c55\" "}
{"level":"info","ts":"2024-05-10T12:55:54.989716Z","caller":"traceutil/trace.go:171","msg":"trace[311884297] transaction","detail":"{read_only:false; response_revision:27446; number_of_response:1; }","duration":"388.330238ms","start":"2024-05-10T12:55:54.60134Z","end":"2024-05-10T12:55:54.98967Z","steps":["trace[311884297] 'process raft request'  (duration: 387.962239ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-10T12:55:54.989997Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:55:54.601294Z","time spent":"388.581138ms","remote":"127.0.0.1:39070","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:27439 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-05-10T12:55:55.226307Z","caller":"traceutil/trace.go:171","msg":"trace[473724780] transaction","detail":"{read_only:false; response_revision:27447; number_of_response:1; }","duration":"592.466343ms","start":"2024-05-10T12:55:54.63378Z","end":"2024-05-10T12:55:55.226246Z","steps":["trace[473724780] 'process raft request'  (duration: 468.952321ms)","trace[473724780] 'compare'  (duration: 94.621863ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-10T12:55:55.227245Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:55:54.633491Z","time spent":"593.54624ms","remote":"127.0.0.1:38984","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:27445 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-05-10T12:55:55.228815Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"138.769399ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-10T12:55:55.24266Z","caller":"traceutil/trace.go:171","msg":"trace[760057985] linearizableReadLoop","detail":"{readStateIndex:34314; appliedIndex:34313; }","duration":"107.590744ms","start":"2024-05-10T12:55:55.090022Z","end":"2024-05-10T12:55:55.197613Z","steps":["trace[760057985] 'read index received'  (duration: 12.591982ms)","trace[760057985] 'applied index is now lower than readState.Index'  (duration: 94.996262ms)"],"step_count":2}
{"level":"info","ts":"2024-05-10T12:55:55.243797Z","caller":"traceutil/trace.go:171","msg":"trace[1913503340] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:27447; }","duration":"139.627498ms","start":"2024-05-10T12:55:55.089866Z","end":"2024-05-10T12:55:55.229494Z","steps":["trace[1913503340] 'agreement among raft nodes before linearized reading'  (duration: 138.854399ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:57:19.402807Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27276}
{"level":"info","ts":"2024-05-10T12:57:19.452102Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":27276,"took":"48.818629ms","hash":1470446946,"current-db-size-bytes":3571712,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1228800,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-05-10T12:57:19.452208Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1470446946,"revision":27276,"compact-revision":26552}
{"level":"warn","ts":"2024-05-10T12:57:58.579051Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:57:58.25999Z","time spent":"319.030335ms","remote":"127.0.0.1:38852","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-05-10T12:57:58.819233Z","caller":"traceutil/trace.go:171","msg":"trace[1774865266] transaction","detail":"{read_only:false; response_revision:27545; number_of_response:1; }","duration":"112.152837ms","start":"2024-05-10T12:57:58.706913Z","end":"2024-05-10T12:57:58.819066Z","steps":["trace[1774865266] 'process raft request'  (duration: 48.950529ms)","trace[1774865266] 'compare'  (duration: 62.123209ms)"],"step_count":2}
{"level":"info","ts":"2024-05-10T12:57:59.159279Z","caller":"traceutil/trace.go:171","msg":"trace[1909694290] transaction","detail":"{read_only:false; response_revision:27546; number_of_response:1; }","duration":"432.755168ms","start":"2024-05-10T12:57:58.726479Z","end":"2024-05-10T12:57:59.159234Z","steps":["trace[1909694290] 'process raft request'  (duration: 432.445169ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-10T12:57:59.159651Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:57:58.726418Z","time spent":"433.010868ms","remote":"127.0.0.1:38984","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:27543 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-05-10T12:57:59.300286Z","caller":"traceutil/trace.go:171","msg":"trace[823720938] linearizableReadLoop","detail":"{readStateIndex:34440; appliedIndex:34438; }","duration":"433.856167ms","start":"2024-05-10T12:57:58.866387Z","end":"2024-05-10T12:57:59.300243Z","steps":["trace[823720938] 'read index received'  (duration: 291.917674ms)","trace[823720938] 'applied index is now lower than readState.Index'  (duration: 141.935893ms)"],"step_count":2}
{"level":"info","ts":"2024-05-10T12:57:59.300867Z","caller":"traceutil/trace.go:171","msg":"trace[1074717111] transaction","detail":"{read_only:false; response_revision:27547; number_of_response:1; }","duration":"529.406228ms","start":"2024-05-10T12:57:58.771428Z","end":"2024-05-10T12:57:59.300834Z","steps":["trace[1074717111] 'process raft request'  (duration: 469.787715ms)","trace[1074717111] 'compare'  (duration: 58.800314ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-10T12:57:59.301116Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:57:58.77137Z","time spent":"529.571428ms","remote":"127.0.0.1:39070","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:27539 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-05-10T12:57:59.333687Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"467.628118ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:424"}
{"level":"warn","ts":"2024-05-10T12:57:59.333781Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"448.947545ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-10T12:57:59.334114Z","caller":"traceutil/trace.go:171","msg":"trace[1181801262] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:27547; }","duration":"468.238917ms","start":"2024-05-10T12:57:58.865686Z","end":"2024-05-10T12:57:59.333925Z","steps":["trace[1181801262] 'agreement among raft nodes before linearized reading'  (duration: 435.763564ms)","trace[1181801262] 'filter and sort the key-value pairs'  (duration: 31.928353ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-10T12:57:59.334262Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"145.562388ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-05-10T12:57:59.334542Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"240.534549ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:889"}
{"level":"info","ts":"2024-05-10T12:57:59.364485Z","caller":"traceutil/trace.go:171","msg":"trace[2096816944] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:27547; }","duration":"270.479205ms","start":"2024-05-10T12:57:59.093954Z","end":"2024-05-10T12:57:59.364433Z","steps":["trace[2096816944] 'agreement among raft nodes before linearized reading'  (duration: 240.364149ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-10T12:57:59.365265Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:57:58.840328Z","time spent":"524.885434ms","remote":"127.0.0.1:38984","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":446,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
{"level":"info","ts":"2024-05-10T12:57:59.365648Z","caller":"traceutil/trace.go:171","msg":"trace[492677440] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:27547; }","duration":"177.128641ms","start":"2024-05-10T12:57:59.188488Z","end":"2024-05-10T12:57:59.365617Z","steps":["trace[492677440] 'agreement among raft nodes before linearized reading'  (duration: 145.700887ms)"],"step_count":1}
{"level":"info","ts":"2024-05-10T12:57:59.33546Z","caller":"traceutil/trace.go:171","msg":"trace[1381658579] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:27547; }","duration":"450.742443ms","start":"2024-05-10T12:57:58.884493Z","end":"2024-05-10T12:57:59.335235Z","steps":["trace[1381658579] 'agreement among raft nodes before linearized reading'  (duration: 449.063445ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-10T12:57:59.366752Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-10T12:57:58.884366Z","time spent":"482.350097ms","remote":"127.0.0.1:38798","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}


==> etcd [62f9612d2ab3] <==


==> kernel <==
 12:58:57 up  9:19,  0 users,  load average: 0.11, 0.76, 0.98
Linux minikube 5.10.207 #1 SMP Thu Apr 18 22:28:35 UTC 2024 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [1f09b3610644] <==
Trace[314044185]: [518.452693ms] [518.452693ms] END
I0510 12:47:42.801398       1 trace.go:236] Trace[1297418170]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:e6936fe5-16d9-44e5-9557-2b32f181b03c,client:172.21.205.249,api-group:apps,api-version:v1,name:mysql-deployment,subresource:,namespace:default,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/default/deployments/mysql-deployment,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (10-May-2024 12:47:42.268) (total time: 531ms):
Trace[1297418170]: ---"About to write a response" 530ms (12:47:42.799)
Trace[1297418170]: [531.054976ms] [531.054976ms] END
I0510 12:47:42.851130       1 trace.go:236] Trace[1083742719]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:06743c62-4bd2-45d2-a8cf-3503feeecb1e,client:172.21.205.249,api-group:apps,api-version:v1,name:php-deployment-5d8b6cd79,subresource:,namespace:default,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/default/replicasets/php-deployment-5d8b6cd79,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (10-May-2024 12:47:42.280) (total time: 570ms):
Trace[1083742719]: ---"About to write a response" 570ms (12:47:42.850)
Trace[1083742719]: [570.232923ms] [570.232923ms] END
I0510 12:47:42.852472       1 trace.go:236] Trace[2134196271]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:848413c7-7ef0-4e3c-859e-84d203f523ee,client:172.21.205.249,api-group:apps,api-version:v1,name:coredns-7db6d8ff4d,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/replicasets/coredns-7db6d8ff4d,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (10-May-2024 12:47:42.281) (total time: 570ms):
Trace[2134196271]: ---"About to write a response" 570ms (12:47:42.852)
Trace[2134196271]: [570.745422ms] [570.745422ms] END
I0510 12:47:42.853406       1 trace.go:236] Trace[58061033]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:9271cf8b-0245-41b6-9097-26c033e8e65a,client:172.21.205.249,api-group:apps,api-version:v1,name:dashboard-metrics-scraper-b5fc48f67,subresource:,namespace:kubernetes-dashboard,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/kubernetes-dashboard/replicasets/dashboard-metrics-scraper-b5fc48f67,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (10-May-2024 12:47:42.282) (total time: 570ms):
Trace[58061033]: ---"About to write a response" 570ms (12:47:42.853)
Trace[58061033]: [570.375623ms] [570.375623ms] END
I0510 12:47:42.887621       1 trace.go:236] Trace[526866321]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:df46be50-1ee5-4741-8d18-eca9f9c83604,client:172.21.205.249,api-group:,api-version:v1,name:kube-dns,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/kube-dns,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (10-May-2024 12:47:42.267) (total time: 618ms):
Trace[526866321]: ---"limitedReadBody succeeded" len:451 31ms (12:47:42.299)
Trace[526866321]: ["GuaranteedUpdate etcd3" audit-id:df46be50-1ee5-4741-8d18-eca9f9c83604,key:/services/endpoints/kube-system/kube-dns,type:*core.Endpoints,resource:endpoints 588ms (12:47:42.299)
Trace[526866321]:  ---"Txn call completed" 572ms (12:47:42.885)]
Trace[526866321]: [618.918356ms] [618.918356ms] END
I0510 12:47:42.893482       1 trace.go:236] Trace[1756787296]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a2db4ff2-509f-40d5-9c1e-db3568bd173c,client:172.21.205.249,api-group:,api-version:v1,name:dashboard-metrics-scraper,subresource:,namespace:kubernetes-dashboard,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/endpoints/dashboard-metrics-scraper,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (10-May-2024 12:47:42.268) (total time: 624ms):
Trace[1756787296]: ---"limitedReadBody succeeded" len:502 44ms (12:47:42.313)
Trace[1756787296]: ["GuaranteedUpdate etcd3" audit-id:a2db4ff2-509f-40d5-9c1e-db3568bd173c,key:/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper,type:*core.Endpoints,resource:endpoints 580ms (12:47:42.313)
Trace[1756787296]:  ---"Txn call completed" 578ms (12:47:42.893)]
Trace[1756787296]: [624.934848ms] [624.934848ms] END
I0510 12:47:42.895072       1 trace.go:236] Trace[913064144]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:274e5335-b59e-48a2-8f58-29b0aca3edf5,client:172.21.205.249,api-group:,api-version:v1,name:kubernetes-dashboard,subresource:,namespace:kubernetes-dashboard,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/endpoints/kubernetes-dashboard,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:endpoint-controller,verb:PUT (10-May-2024 12:47:42.280) (total time: 614ms):
Trace[913064144]: ---"limitedReadBody succeeded" len:488 33ms (12:47:42.314)
Trace[913064144]: ["GuaranteedUpdate etcd3" audit-id:274e5335-b59e-48a2-8f58-29b0aca3edf5,key:/services/endpoints/kubernetes-dashboard/kubernetes-dashboard,type:*core.Endpoints,resource:endpoints 580ms (12:47:42.314)
Trace[913064144]:  ---"Txn call completed" 567ms (12:47:42.894)]
Trace[913064144]: [614.634362ms] [614.634362ms] END
I0510 12:47:42.947692       1 trace.go:236] Trace[1289462969]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:375a0ffd-1842-4712-aa47-362eb03df9a4,client:172.21.205.249,api-group:apps,api-version:v1,name:mysql-deployment-7985ff9c55,subresource:,namespace:default,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/default/replicasets/mysql-deployment-7985ff9c55,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (10-May-2024 12:47:42.283) (total time: 663ms):
Trace[1289462969]: ---"About to write a response" 663ms (12:47:42.946)
Trace[1289462969]: [663.855895ms] [663.855895ms] END
I0510 12:55:55.229821       1 trace.go:236] Trace[1006879278]: "Update" accept:application/json, */*,audit-id:3a53260f-36c7-439a-9430-9d9d2d861846,client:172.21.205.249,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (10-May-2024 12:55:54.618) (total time: 611ms):
Trace[1006879278]: ["GuaranteedUpdate etcd3" audit-id:3a53260f-36c7-439a-9430-9d9d2d861846,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 610ms (12:55:54.619)
Trace[1006879278]:  ---"Txn call completed" 606ms (12:55:55.229)]
Trace[1006879278]: [611.594115ms] [611.594115ms] END
I0510 12:57:58.431330       1 trace.go:236] Trace[855970985]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3a08913f-c2a0-4d3a-bfec-464e4d59f178,client:172.21.205.249,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (10-May-2024 12:57:57.901) (total time: 517ms):
Trace[855970985]: ["GuaranteedUpdate etcd3" audit-id:3a08913f-c2a0-4d3a-bfec-464e4d59f178,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 517ms (12:57:57.901)
Trace[855970985]:  ---"About to Encode" 112ms (12:57:58.014)
Trace[855970985]:  ---"Txn call completed" 341ms (12:57:58.355)]
Trace[855970985]: ---"Write to database call succeeded" len:477 15ms (12:57:58.418)
Trace[855970985]: [517.818945ms] [517.818945ms] END
I0510 12:57:58.834567       1 trace.go:236] Trace[871690892]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/172.21.205.249,type:*v1.Endpoints,resource:apiServerIPInfo (10-May-2024 12:57:57.839) (total time: 994ms):
Trace[871690892]: ---"initial value restored" 387ms (12:57:58.226)
Trace[871690892]: ---"Transaction prepared" 398ms (12:57:58.625)
Trace[871690892]: ---"Txn call completed" 196ms (12:57:58.822)
Trace[871690892]: [994.330849ms] [994.330849ms] END
I0510 12:57:59.225893       1 trace.go:236] Trace[2104774740]: "Update" accept:application/json, */*,audit-id:9dbe8c8a-cbc4-4d28-9638-3ebf567a782b,client:172.21.205.249,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (10-May-2024 12:57:58.384) (total time: 840ms):
Trace[2104774740]: ---"limitedReadBody succeeded" len:1354 111ms (12:57:58.496)
Trace[2104774740]: ["GuaranteedUpdate etcd3" audit-id:9dbe8c8a-cbc4-4d28-9638-3ebf567a782b,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 725ms (12:57:58.500)
Trace[2104774740]:  ---"About to Encode" 76ms (12:57:58.577)
Trace[2104774740]:  ---"Txn call completed" 647ms (12:57:59.225)]
Trace[2104774740]: [840.814773ms] [840.814773ms] END
I0510 12:57:59.336978       1 trace.go:236] Trace[1053643426]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0750790f-643a-40cb-a470-e996d4d29f67,client:127.0.0.1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (10-May-2024 12:57:58.725) (total time: 611ms):
Trace[1053643426]: ---"limitedReadBody succeeded" len:605 31ms (12:57:58.756)
Trace[1053643426]: ["GuaranteedUpdate etcd3" audit-id:0750790f-643a-40cb-a470-e996d4d29f67,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 579ms (12:57:58.757)
Trace[1053643426]:  ---"Txn call completed" 567ms (12:57:59.336)]
Trace[1053643426]: [611.343708ms] [611.343708ms] END
I0510 12:57:59.398636       1 trace.go:236] Trace[422697824]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:dfde2d55-de85-4853-a22e-81a94e2bfa61,client:127.0.0.1,api-group:,api-version:v1,name:kubernetes,subresource:,namespace:default,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:GET (10-May-2024 12:57:58.837) (total time: 560ms):
Trace[422697824]: ---"About to write a response" 560ms (12:57:59.398)
Trace[422697824]: [560.628283ms] [560.628283ms] END


==> kube-apiserver [53f7ae0b3deb] <==


==> kube-controller-manager [81544f66137f] <==


==> kube-controller-manager [ed6bb1150e68] <==
I0510 12:47:40.919587       1 controllermanager.go:711] "Controller is disabled by a feature gate" controller="service-cidr-controller" requiredFeatureGates=["MultiCIDRServiceAllocator"]
I0510 12:47:40.918972       1 controller.go:170] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
I0510 12:47:40.920399       1 shared_informer.go:313] Waiting for caches to sync for ephemeral
I0510 12:47:41.205119       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0510 12:47:41.320787       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0510 12:47:41.321443       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0510 12:47:41.321884       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0510 12:47:41.324056       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0510 12:47:41.336009       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0510 12:47:41.515188       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0510 12:47:41.526535       1 shared_informer.go:320] Caches are synced for PV protection
I0510 12:47:41.529048       1 shared_informer.go:320] Caches are synced for node
I0510 12:47:41.529387       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0510 12:47:41.529466       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0510 12:47:41.529568       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0510 12:47:41.529657       1 shared_informer.go:320] Caches are synced for cidrallocator
I0510 12:47:41.594275       1 shared_informer.go:320] Caches are synced for expand
I0510 12:47:41.594366       1 shared_informer.go:320] Caches are synced for TTL
I0510 12:47:41.609463       1 shared_informer.go:320] Caches are synced for cronjob
I0510 12:47:41.622947       1 shared_informer.go:320] Caches are synced for namespace
I0510 12:47:41.623389       1 shared_informer.go:320] Caches are synced for service account
I0510 12:47:41.642322       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0510 12:47:41.659289       1 shared_informer.go:320] Caches are synced for TTL after finished
I0510 12:47:41.723048       1 shared_informer.go:320] Caches are synced for ReplicationController
I0510 12:47:41.723319       1 shared_informer.go:320] Caches are synced for persistent volume
I0510 12:47:41.723393       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0510 12:47:41.724184       1 shared_informer.go:320] Caches are synced for PVC protection
I0510 12:47:41.724351       1 shared_informer.go:320] Caches are synced for ephemeral
I0510 12:47:41.724826       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0510 12:47:41.736833       1 shared_informer.go:320] Caches are synced for attach detach
I0510 12:47:41.737150       1 shared_informer.go:320] Caches are synced for daemon sets
I0510 12:47:41.737173       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0510 12:47:41.769575       1 shared_informer.go:320] Caches are synced for GC
I0510 12:47:41.754433       1 shared_informer.go:320] Caches are synced for deployment
I0510 12:47:41.769913       1 shared_informer.go:320] Caches are synced for HPA
I0510 12:47:41.768534       1 shared_informer.go:320] Caches are synced for stateful set
I0510 12:47:41.789023       1 shared_informer.go:320] Caches are synced for job
I0510 12:47:41.768566       1 shared_informer.go:320] Caches are synced for disruption
I0510 12:47:41.816306       1 shared_informer.go:320] Caches are synced for taint
I0510 12:47:41.817120       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0510 12:47:41.817710       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0510 12:47:41.818248       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0510 12:47:41.818885       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0510 12:47:41.819438       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-deployment-7985ff9c55" duration="101.5µs"
I0510 12:47:41.820090       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-5d8b6cd79" duration="100.5µs"
I0510 12:47:41.820709       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-d799bb74f" duration="221.4µs"
I0510 12:47:41.821375       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="151µs"
I0510 12:47:41.822281       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="270.3µs"
I0510 12:47:41.832653       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="173.1µs"
I0510 12:47:41.849808       1 shared_informer.go:320] Caches are synced for crt configmap
I0510 12:47:41.850825       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0510 12:47:41.880379       1 shared_informer.go:320] Caches are synced for endpoint
I0510 12:47:41.880961       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0510 12:47:41.928826       1 shared_informer.go:320] Caches are synced for resource quota
I0510 12:47:41.930249       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0510 12:47:41.946186       1 shared_informer.go:320] Caches are synced for resource quota
I0510 12:47:42.024625       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0510 12:47:42.140832       1 shared_informer.go:320] Caches are synced for garbage collector
I0510 12:47:42.141074       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0510 12:47:42.232181       1 shared_informer.go:320] Caches are synced for garbage collector


==> kube-proxy [88e6485218e6] <==
I0510 12:47:27.195066       1 server_linux.go:69] "Using iptables proxy"
I0510 12:47:27.276612       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["172.21.205.249"]
I0510 12:47:27.484312       1 server_linux.go:143] "No iptables support for family" ipFamily="IPv6"
I0510 12:47:27.484683       1 server.go:661] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0510 12:47:27.485044       1 server_linux.go:165] "Using iptables Proxier"
I0510 12:47:27.513002       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0510 12:47:27.524547       1 server.go:872] "Version info" version="v1.30.0"
I0510 12:47:27.525220       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0510 12:47:27.551754       1 config.go:192] "Starting service config controller"
I0510 12:47:27.551786       1 shared_informer.go:313] Waiting for caches to sync for service config
I0510 12:47:27.552322       1 config.go:101] "Starting endpoint slice config controller"
I0510 12:47:27.552345       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0510 12:47:27.555542       1 config.go:319] "Starting node config controller"
I0510 12:47:27.555567       1 shared_informer.go:313] Waiting for caches to sync for node config
I0510 12:47:27.761685       1 shared_informer.go:320] Caches are synced for service config
I0510 12:47:27.764108       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0510 12:47:27.795636       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [8c27d495d41d] <==


==> kube-scheduler [05b4a23545c4] <==
I0510 12:47:18.311749       1 serving.go:380] Generated self-signed cert in-memory
W0510 12:47:23.093585       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0510 12:47:23.093724       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0510 12:47:23.093787       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0510 12:47:23.093854       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0510 12:47:23.174952       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0510 12:47:23.177576       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0510 12:47:23.184423       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0510 12:47:23.184984       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0510 12:47:23.192209       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0510 12:47:23.187146       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0510 12:47:23.295791       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [b2944bdb3ce6] <==
I0510 12:47:05.771673       1 serving.go:380] Generated self-signed cert in-memory
W0510 12:47:06.555907       1 authentication.go:368] Error looking up in-cluster authentication configuration: Get "https://172.21.205.249:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": dial tcp 172.21.205.249:8443: connect: connection refused
W0510 12:47:06.556112       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0510 12:47:06.556139       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0510 12:47:06.579828       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0510 12:47:06.579880       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0510 12:47:06.610322       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0510 12:47:06.610573       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0510 12:47:06.610641       1 shared_informer.go:316] unable to sync caches for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0510 12:47:06.610678       1 configmap_cafile_content.go:210] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0510 12:47:06.611940       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0510 12:47:06.611985       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0510 12:47:06.612072       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0510 12:47:06.612435       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0510 12:47:06.625078       1 server.go:214] "waiting for handlers to sync" err="context canceled"
E0510 12:47:06.625306       1 run.go:74] "command failed" err="finished without leader elect"


==> kubelet <==
May 10 12:47:24 minikube kubelet[120827]: I0510 12:47:24.135062  120827 scope.go:117] "RemoveContainer" containerID="ff88b2e23f2eb7526b7c961b4afe49234f82bfb388c687be6741ebc918d051d3"
May 10 12:47:24 minikube kubelet[120827]: I0510 12:47:24.136495  120827 scope.go:117] "RemoveContainer" containerID="491815b8ba93862a910c3028e9dbe09b7ce00a54a2653f90530704953218b3d4"
May 10 12:47:25 minikube kubelet[120827]: I0510 12:47:25.877376  120827 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="58e89b16e4475bbb2e96523355f4bb08a955d089045cc1445911bb58277b1349"
May 10 12:47:26 minikube kubelet[120827]: I0510 12:47:26.395729  120827 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="df55ba40302f2488a125bfe8828750536322f8d39326a9500861a79a7f3ceabb"
May 10 12:48:13 minikube kubelet[120827]: E0510 12:48:13.089198  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:48:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:48:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:48:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:48:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:49:13 minikube kubelet[120827]: E0510 12:49:13.084808  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:49:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:49:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:49:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:49:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:50:13 minikube kubelet[120827]: E0510 12:50:13.089415  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:50:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:50:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:50:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:50:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:51:13 minikube kubelet[120827]: E0510 12:51:13.121374  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:51:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:51:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:51:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:51:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:52:13 minikube kubelet[120827]: E0510 12:52:13.105789  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:52:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:52:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:52:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:52:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:53:13 minikube kubelet[120827]: E0510 12:53:13.113306  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:53:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:53:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:53:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:53:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:54:13 minikube kubelet[120827]: E0510 12:54:13.114273  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:54:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:54:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:54:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:54:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:55:13 minikube kubelet[120827]: E0510 12:55:13.109730  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:55:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:55:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:55:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:55:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:56:13 minikube kubelet[120827]: E0510 12:56:13.084731  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:56:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:56:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:56:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:56:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:57:13 minikube kubelet[120827]: E0510 12:57:13.086847  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:57:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:57:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:57:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:57:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 10 12:57:58 minikube kubelet[120827]: E0510 12:57:58.738885  120827 kubelet.go:2511] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.742s"
May 10 12:58:13 minikube kubelet[120827]: E0510 12:58:13.130177  120827 iptables.go:577] "Could not set up iptables canary" err=<
May 10 12:58:13 minikube kubelet[120827]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 10 12:58:13 minikube kubelet[120827]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 10 12:58:13 minikube kubelet[120827]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 10 12:58:13 minikube kubelet[120827]:  > table="nat" chain="KUBE-KUBELET-CANARY"


==> kubernetes-dashboard [54164f977423] <==
2024/05/10 12:49:18 Getting list of all replication controllers in the cluster
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 [2024-05-10T12:49:18Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:18 Getting list of all pet sets in the cluster
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 Getting pod metrics
2024/05/10 12:49:18 [2024-05-10T12:49:18Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:18 [2024-05-10T12:49:18Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 [2024-05-10T12:49:18Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 [2024-05-10T12:49:18Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:18 received 0 resources from sidecar instead of 2
2024/05/10 12:49:18 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:18 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:18 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:18 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:18 [2024-05-10T12:49:18Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of namespaces
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of all cron jobs in the cluster
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of all deployments in the cluster
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of all replica sets in the cluster
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of all jobs in the cluster
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of all replication controllers in the cluster
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of all pods in the cluster
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:49:21 Getting list of all pet sets in the cluster
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 Getting pod metrics
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 received 0 resources from sidecar instead of 2
2024/05/10 12:49:21 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:21 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:21 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:21 Skipping metric because of error: Metric label not set.
2024/05/10 12:49:21 [2024-05-10T12:49:21Z] Outcoming response to 127.0.0.1 with 200 status code


==> kubernetes-dashboard [ff88b2e23f2e] <==
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Incoming HTTP/1.1 GET /api/v1/settings/pinner request from 127.0.0.1: 
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Incoming HTTP/1.1 GET /api/v1/settings/global request from 127.0.0.1: 
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Incoming HTTP/1.1 GET /api/v1/systembanner request from 127.0.0.1: 
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/10 12:03:05 Getting list of namespaces
2024/05/10 12:03:05 [2024-05-10T12:03:05Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 Getting list of all cron jobs in the cluster
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 Getting list of all deployments in the cluster
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 Getting list of all replica sets in the cluster
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 Getting list of all jobs in the cluster
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 Getting list of all pods in the cluster
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 Getting list of all replication controllers in the cluster
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:06 Getting list of all pet sets in the cluster
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 Getting pod metrics
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 received 0 resources from sidecar instead of 2
2024/05/10 12:03:06 Skipping metric because of error: Metric label not set.
2024/05/10 12:03:06 Skipping metric because of error: Metric label not set.
2024/05/10 12:03:06 Skipping metric because of error: Metric label not set.
2024/05/10 12:03:06 Skipping metric because of error: Metric label not set.
2024/05/10 12:03:06 [2024-05-10T12:03:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:09 [2024-05-10T12:03:09Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/10 12:03:09 Getting list of namespaces
2024/05/10 12:03:09 [2024-05-10T12:03:09Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:09 [2024-05-10T12:03:09Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/10 12:03:09 Getting list of all cron jobs in the cluster
2024/05/10 12:03:09 [2024-05-10T12:03:09Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:09 [2024-05-10T12:03:09Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/10 12:03:09 [2024-05-10T12:03:09Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [3199012c1620] <==
I0510 12:47:25.844437       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0510 12:47:25.927987       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0510 12:47:25.933889       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0510 12:47:41.386097       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0510 12:47:41.386900       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_6ada0751-66d8-4b0a-916d-ab5797b37b2b!
I0510 12:47:41.398955       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"57971052-ecf0-41e1-a6f9-b6e11d98b8da", APIVersion:"v1", ResourceVersion:"27045", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_6ada0751-66d8-4b0a-916d-ab5797b37b2b became leader
I0510 12:47:41.608339       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_6ada0751-66d8-4b0a-916d-ab5797b37b2b!


==> storage-provisioner [491815b8ba93] <==
I0510 12:47:02.628140       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0510 12:47:02.637960       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

